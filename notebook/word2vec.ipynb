{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.50128876,  0.84007629, -1.48067359])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#输入矩阵， 语料库中字母的数量是5，使用三维向量表示一个字母\n",
    "inputVerctors = np.random.randn(5, 3)\n",
    "#输出矩阵\n",
    "outputVectors = np.random.randn(5, 3)\n",
    "#句子\n",
    "sentence = ['a', 'e', 'd', 'b', 'd', 'c', 'd', 'e', 'e', 'c', 'a']\n",
    "#中心字母\n",
    "centerword = 'c'\n",
    "#上下文字母\n",
    "context = ['a', 'e', 'd', 'b', 'd', 'd', 'e', 'e', 'c', 'a']\n",
    "#用于映射字母在输入输出矩阵中的索引\n",
    "tokens = dict([('a', 0), ('b', 1), ('c', 2), ('d', 3), ('e', 4)])\n",
    "\n",
    "inputVerctors[tokens['c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始输入矩阵：\n",
      "[[ 0.23572163  0.44846602 -0.14915017]\n",
      " [ 1.50655112  0.00358611  1.31050247]\n",
      " [-0.50128876  0.84007629 -1.48067359]\n",
      " [-0.01938465  0.96989823 -0.96163808]\n",
      " [ 0.05383894  0.70152992 -1.20296992]]\n",
      "原始输出矩阵：\n",
      "[[ 0.16207975 -0.2206265  -0.17491369]\n",
      " [-0.28257026 -1.37573974  1.16710697]\n",
      " [ 1.02108109 -1.1730129   1.1193676 ]\n",
      " [ 2.17922249 -1.76492886 -0.50243915]\n",
      " [ 0.38828171  1.79918973  0.05192168]]\n",
      "更新后的输入矩阵：\n",
      "[[ 0.23572163  0.44846602 -0.14915017]\n",
      " [ 1.50655112  0.00358611  1.31050247]\n",
      " [-0.50128876  0.84007629 -1.48067359]\n",
      " [-0.01938465  0.96989823 -0.96163808]\n",
      " [ 0.05383894  0.70152992 -1.20296992]]\n",
      "更新后的输出矩阵：\n",
      "[[ 0.16260495 -0.22150666 -0.17336237]\n",
      " [-0.28689816 -1.36848689  1.15432349]\n",
      " [ 1.01652165 -1.16537204  1.10590023]\n",
      " [ 2.16588731 -1.74258132 -0.54182773]\n",
      " [ 0.40997904  1.76282864  0.1160098 ]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    orig_shape = x.shape\n",
    "    #根据输入类型是矩阵还是向量分别计算softmax\n",
    "    if len(x.shape) > 1:\n",
    "        #矩阵\n",
    "        tmp = np.max(x, axis=1) #得到每行的最大值，用于缩放每行的元素，避免溢出\n",
    "        x -= tmp.reshape((x.shape[0], 1)) #使每行减去所在行的最大值（广播运算）\n",
    "        \n",
    "        x = np.exp(x) # 第一步，计算所有以e为底的x次幂\n",
    "        tmp = np.sum(x, axis=1) #将每行求和并保存\n",
    "        x /= tmp.reshape((x.shape[0], 1)) #所有元素所在行的元素和（广播运算）\n",
    "    else:\n",
    "        tmp = np.max(x) #得到最大值\n",
    "        x -= tmp # 利用最大值缩放数据\n",
    "        x = np.exp(x) #对所有元素求以e为底的x次幂\n",
    "        tmp = np.sum(x) #求元素和\n",
    "        x /= tmp #求softmax\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.true_divide(1, 1 + np.exp(-x))\n",
    "\n",
    "# 可以证明：sigmoid函数关于输入x的导数等于`sigmoid(x)(1-sigmoid(x))`\n",
    "def sigmoid_grad(s):\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, ouputVectorts):\n",
    "    v_hat = predicted #中心词向量\n",
    "    z = np.dot(outputVectors, v_hat) #预测得分\n",
    "    y_hat = softmax(z) #预测输出y_hat\n",
    "    \n",
    "    cost = -np.log(y_hat[target]) #计算代价\n",
    "    \n",
    "    z = y_hat.copy()\n",
    "    z[target] -= 1.0\n",
    "    grad = np.outer(z, v_hat) # 计算中心词的梯度\n",
    "    gradPred = np.dot(outputVectors.T, z) #计算输出词向量矩阵的梯度\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "## 前向传播\n",
    "def skipgram(currentWord, contextWords, tokens, inputVectors, ouputVectors):\n",
    "    #初始化变量\n",
    "    cost = 0\n",
    "    gradIn = np.zeros(inputVerctors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    \n",
    "    cword_idx = tokens[currentWord] #得到中心词的索引\n",
    "    v_hat = inputVerctors[cword_idx] #得到中心词的词向量\n",
    "    \n",
    "    #循环预测上下文中每个字母\n",
    "    for j in contextWords:\n",
    "        u_idx = tokens[j]\n",
    "        c_cost, c_grad_in, c_grad_out = softmaxCostAndGradient(v_hat, u_idx, outputVectors)\n",
    "        cost += c_cost #所有代价求和\n",
    "        gradIn[cword_idx] + c_grad_in # 中心词向量梯度求和\n",
    "        gradOut += c_grad_out #输出词向量矩阵梯度求和\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "c, gin, gout = skipgram(centerword, context, tokens, inputVerctors, outputVectors)\n",
    "step = 0.01 #学习速率\n",
    "print '原始输入矩阵：\\n', inputVerctors\n",
    "print '原始输出矩阵：\\n', outputVectors\n",
    "inputVerctors -= step * gin\n",
    "outputVectors -= step * gout\n",
    "print '更新后的输入矩阵：\\n', inputVerctors\n",
    "print '更新后的输出矩阵：\\n', outputVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
